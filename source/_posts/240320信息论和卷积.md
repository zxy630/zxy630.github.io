---
title: 信息论和卷积
date: 2024-03-20 08:59:00
author: 小粥同学
tags: 深度学习
---

## 信息论基础概念

1. 信息量：$-log(px(x))$，即一件事情发生的概率越小，其所包含的信息量越大
2. 熵：事件所包含信息量的期望，熵越大越混乱
3. 交叉熵：Entropy Loss，$H(p,q)=-\sum_{i=1}^np(x_i)log(q(x_i))$
4. KL散度：测量近似分布时损失的信息，可以用来找最优分布，但不是对称的，不是距离
5. JS散度：对称，梯度为0，两个分布完全不重叠时JS为常数2
6. HD95：衡量2个集合间距离，越小模型性能越好，计算时滤去5%的离群点，保留点集距离的最大值，故算是衡量边界点

## 各种卷积
1. 卷积（下采样）：降维，将卷积块与目标一一乘积求和
2. 反卷积（上采样）：保留映射中相对位置关系信息，但不是复原，filter大小不能被stride整除的话就会有不均匀重叠，也就是棋盘格伪影
3. 扩张卷积（膨胀、空洞卷积、dilate）：扩大感受野，可以在多尺度中使用，膨胀系数为1时就是普通卷积
4. 可分离卷积（separate）：空间或深度，可以大幅减少卷积参数，但并不是所有卷积核都可分，使用时要限制类型，故可能会影响训练效果
5. 扁平卷积：
6. 分组卷积：拆分以达到多GPU训练